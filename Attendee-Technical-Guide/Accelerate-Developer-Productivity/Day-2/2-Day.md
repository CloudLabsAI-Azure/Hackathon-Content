# Activate Gen AI: 2-Day Attendee Guide

Welcome to the Activate Gen AI Hackathon. This two-day event is designed to immerse you in the world of AI application development, harnessing the power and versatility of the Azure OpenAI Service.

## Introduction

Delve into the world of Azure OpenAI. This event is designed to enhance your AI application development skills, regardless of your current experience level. Through the self-administered challenges, you'll apply your knowledge, build practical solutions, and acquire a deeper understanding of implementing AI solutions using Azure OpenAI.

## Learning Objectives

By participating in this hackathon, you will learn how to:

- Understand GenAI concepts and how they can be used to build chat applications.
- Learn how to configure the Azure OpenAI service and use Azure AI Search to build private OpenAI with your own data.
- Gain practical experience in implementing a chat app architecture with Terraform.
- Explore features of chat apps, such as multilingual queries and advanced chat response settings.
- Apply GenAI to real-world scenarios, including increased user interactions and dynamic document processing.
- Operationalize AI-enabled applications with enterprise-level monitoring and logging.
- Scale AI-enabled applications with enterprise-level load balancing.


## Hackathon Format: Challenge-Based

This hackathon adopts a challenge-based format, offering you a unique opportunity to learn while dealing with practical problems. Each challenge includes one or more self-contained tasks designed to test and enhance your skills in specific aspects of AI app development. You will approach these challenges by:

- Analyzing the problem statement.
- Strategizing your approach to find the most effective solution.
- Leveraging the provided lab environment and Azure Open AI tools.
- Collaborating with peers to refine and implement your solutions.

## Challenges Overview

# Activate GenAI with Azure: 2-Day Attendee Guide

Welcome to the Activate GenAI Hackathon! Today, you're set to dive into the transformative world of AI, with a focus on utilizing the power of Azure OpenAI services. Prepare yourself for a day of intense learning, innovation, and hands-on experience that will elevate your understanding of AI integration in application development.

## Introduction

Your quest is to innovate for the future at the fictitious enterprise, GenTech Inc. Your challenge? To integrate next-generation AI capabilities into GenTech's ecosystem, enhancing their operations and customer engagement through intelligent application development.


## Learning Objectives

By participating in this hackathon, you will learn how to:

- Gain practical experience in deploying and managing Azure OpenAI services, including Large Language Models (LLMs).
- Develop competencies in implementing intelligent document search capabilities using Azure AI.
- Acquire the ability to set up and deploy a conversational AI interface with a sample chat application.
- Enhance your problem-solving skills by addressing real-world scenarios with AI-driven solutions.
- Collaborate with other participants to tackle challenges and share insights.

## Hackathon Format: Challenge-Based
This hackathon adopts a challenge-based format, offering you a unique opportunity to learn while dealing with practical problems. Each challenge includes one or more self-contained tasks designed to test and enhance your skills in specific aspects of AI app development. You will approach these challenges by:

- Analyzing the problem statement.
- Strategizing your approach to find the most effective solution.
- Leveraging the provided lab environment and Azure AI services.
- Collaborating with peers to refine and implement your solutions.
  
## Challenges

1. **Challenge 01: Deploy Azure OpenAI Service and LLM Models**
   - Begin your journey by deploying the Azure OpenAI Service and integrating a Large Language Model (LLM). This will serve as the foundation for advanced linguistic intelligence in your applications.

     
2. **Challenge 02: Implement Document Search with Azure AI Search**
   - Construct an Azure AI Search solution to enable sophisticated document handling. Upload, index, and tailor the search experience using VS Code and Azure. This lays the groundwork for document-based questioning essential for Retriever-Augmented Generation (RAG) in OpenAI.

3. **Challenge 03: Deploy NVIDIA NIM on Azure**
   - Deploy NVIDIA NIM to Azure to harness powerful, scalable generative AI model hosting. This guide walks you through setting up NVIDIA NIM—a suite of optimized microservices specifically designed for deploying and managing generative AI models on NVIDIA GPUs. Using Azure and NGC (NVIDIA GPU Cloud), you will deploy models like meta/llama-3.1-8b-instruct with streamlined configuration, security, and performance features. By leveraging the NIM containerization on Azure, you'll establish an efficient and secure environment for real-time AI inference that is suitable for complex generative AI applications.
            
4. **Challenge 04: Deploy an AI-Powered Chat App**
   - Utilize Terraform to deploy a sample chat application on Azure, automating the provisioning of all necessary resources. This challenge will demonstrate your ability to get an application up and running in the cloud.
                         
5. **Challenge 05: Serverless Document Batch Processing**
   - Embrace the power of serverless computing for batch-processing documents. Analyze documents using Azure Document Intelligence Services and process them with OpenAI. 
     
6. **Challenge 06: Implement Monitoring and Logging of Azure OpenAI Using API Management Service**
   - Operationalize your solution by setting up monitoring and logging. Use Azure Monitor, Log Analytics, and APIM to gain insights and maintain the robustness of your AI services.

Feel free to explore the challenges, learn, and have fun during this hackathon! If you have any questions, don't hesitate to reach out to your coach.

Happy hacking!

# Hackathon Mission and Use-Case
Welcome to the Activate GenAI Hackathon, an initiative brought to life by Contoso Ltd. Our mission is to unlock the capabilities of AI and Large Language Models (LLMs) to redefine the landscape of enterprise solutions. This hackathon is a critical milestone in our quest for AI innovation, pushing us to discover groundbreaking applications that resonate with our strategic vision. We inspire participants to think beyond the conventional, leverage AI technologies in transformative ways, and contribute to setting new benchmarks in AI-driven enterprise solutions.

As a forward-thinking entity in the tech sphere, Contoso Ltd. acknowledges the extraordinary impact that AI, particularly Azure OpenAI services, can have on business processes and customer interaction. Our ambition for this hackathon is to harness, experiment with, and execute AI-infused solutions that integrate flawlessly with our existing frameworks, boosting operational effectiveness and customer satisfaction.

Your mandate, as avant-garde developers, is to navigate the vast expanse of AI, harnessing Azure OpenAI’s sophisticated tools. We are entrusting you with the task of ideating and developing applications that can:
1. **Integrate Azure OpenAI into Contoso's Corporate Ecosystem**
    - Deploy and experiment with LLMs to enrich GenTech's communication channels, focusing on interpreting, analyzing, and summarizing internal and external communications for more streamlined and impactful exchanges.
2. **Implement Intelligent Document Search**
    - Engineer innovative tools that allow for efficient indexing and searching of vast document repositories, enabling rapid retrieval of information and knowledge discovery, crucial for informed decision-making within GenTech.
3. **Establish AI-Powered Chat Interactions**
    -  Deploy and refine conversational AI interfaces within Contoso's digital ecosystem, ensuring they can handle intricate, multilingual conversations with finesse, thereby elevating the user experience on our platforms.
4. **Establish AI-Powered Personalized Chat Experience with Private Data**
    -  Enhance the conversational AI interfaces by equipping them with personalized data, such as building a bot for employees to inquire about the company's benefits.

Throughout the challenges of this hackathon, you will interact with state-of-the-art technologies essential for realizing Contoso's objectives. You will be equipped with the necessary arsenal of tools and services to effectively bring these use cases to fruition.

At Contoso Ltd., we are convinced that the integration of Azure OpenAI into our applications is not merely an enhancement of our workflow but a catalyst for delivering exceptional value to our stakeholders. This hackathon is more than a competition; it represents a pivotal leap towards an era where AI is an integral part of every solution we craft. Join us in this enthralling venture to extend the boundaries of what's achievable with AI in enterprise application development.

# Technical Prerequisites:

Before you begin, ensure that you have the following prerequisites installed and configured in CloudLabs provided Integrated Environment (JumpVM), which you are going to use throughout the hackathon:
 
 > **Note**: Pre-requisites are already set up in the provided environment. If you're using your personal computer or laptop, please make sure that all necessary prerequisites are installed to complete this hackathon.
      
1. **Development IDE:** Choose a preferred integrated development environment (IDE) for coding. Popular choices include Visual Studio Code, Visual Studio, or any IDE that supports your programming language.

1. **Node.js:** Install [Node.js 14+](https://nodejs.org/en/download/)

1. **Git:** Install Git to clone and contribute to the repository. You can download Git from [here](https://git-scm.com/).

1. **Azure CLI:** The Azure Command-Line Interface (CLI) is useful for managing Azure resources from the command line. Install it [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli).

1. **Bicep:** [ Azure Bicep](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/install)

1. **Python:** Install [Python 3.12.1](https://www.python.org/downloads)
     * **Important**: Python and the pip package manager must be in the path on Windows for the setup scripts to work.
     * **Important**: Ensure you can run `python --version` from the console. On Ubuntu, you might need to run `sudo apt install python-is-python3` to link `python` to `python3`.

1. Install [Visual Studio Code](https://code.visualstudio.com/download).

Happy hacking!

## Challenge 01: Deploy Azure OpenAI Service and LLM Models

### Estimated Time: 30 minutes

## Introduction

**Azure OpenAI Service** provides REST API access to OpenAI's powerful language models, including the GPT-4, GPT-4 Turbo with Vision, `gpt-35-turbo`, and Embeddings model series. In addition, the new `GPT-4` and `gpt-35-turbo` model series have now reached general availability.

A **Large Language Model (LLM)** is a deep learning algorithm that can perform a variety of natural language processing (NLP) tasks. Large language models use transformer models and are trained using massive datasets—hence, large. This enables them to recognize, translate, predict, or generate text or other content.

**Contoso Ltd.**, a leading technological firm, is seeking to enhance its product support operations. They receive a vast number of queries daily, which results in longer waiting times and decreased customer satisfaction. To address this, Contoso is planning to implement an AI-powered solution that can handle customer inquiries effectively and efficiently.

They have chosen to deploy Azure OpenAI Service along with its Large Language Models (LLM), like `gpt-35-turbo` and `text-embedding-ada-002`. These models are known for their capability in processing and generating human-like text, making them ideal for this application.

As a part of this challenge, your task is to create an Azure OpenAI service and deploy Large Language Models (LLM). The LLMs include **gpt-35-turbo** and **text-embedding-ada-002**.

### Accessing the Azure portal

1. To access the Azure portal, open a private/incognito window in your browser and navigate to the Azure Portal.

1. On the **Sign in to Microsoft Azure tab**, you will see a login screen. Enter the following email/username, and then click on **Next**.

   - **Email/Username:** <inject key="AzureAdUserEmail"></inject>

1. Now enter the following password and click on **Sign in**.

   - **Password:** <inject key="AzureAdUserPassword"></inject>

1. If you see the pop-up **Stay Signed in?**, click No.

1. If you see the pop-up **You have free Azure Advisor recommendations!**, close the window to continue with the challenge.

1. If a **Welcome to Microsoft Azure** pop-up window appears, click **Maybe Later** to skip the tour.

## Prerequisites

Make sure you have the following from the CloudLabs-provided integrated environment:

> Note: Prerequisites are already set up in the CloudLabs-provided environment. If you're using your personal computer or laptop, please make sure that all necessary prerequisites are installed to complete this hackathon.

  - [Azure Subscription](https://azure.microsoft.com/en-us/free/)
  - [Azure OpenAI](https://aka.ms/oai/access) access is available with the following models:
    - gpt-35-turbo
    - text-embedding-ada-002

## Challenge Objectives:

1. **Azure OpenAI Service Deployment:**
   - Set up an Azure OpenAI Service instance with SKU size Standard `S0`.
   - Deploy it in the existing resource group named - **<inject key="Resource Group Name"/>**
   - Deploy the resource in the **East US** region.
   - Obtain the necessary Azure OpenAI Key and Endpoint.
   - Please ensure the Azure OpenAI Service name follows this format: **OpenAI-xxxxxx**, where xxxxxx should be replaced with your specific **Deployment ID**.

     <validation step="ccff4a0f-eb81-479e-a774-00cc5a664eeb" />

2. **Deploy Large Language Models (LLM):**
   - Azure OpenAI provides a web-based portal named **Azure AI Foundry Portal** that you can use to deploy, manage, and explore models. You'll start your exploration of Azure OpenAI by using Azure AI Foundry Portal to deploy a model.
   - Launch Azure AI Foundry Portal from the overview pane and deploy two OpenAI models, i.e., `gpt-35-turbo` and `text-embedding-ada-002`, with a TPM capacity of 20k.

     > **Note:** Ensure you deploy **gpt-35-turbo** model with **version : 0125**.


## Success Criteria:

- Verify that the Azure OpenAI Service is successfully deployed in the existing resource group - <inject key="Resource Group Name"/>.
- Verify that the Large Language Models (LLM), `gpt-35-turbo` and `text-embedding-ada-002`, are successfully deployed with the Azure OpenAI Service.

## Additional Resources:

- Refer to the [Azure OpenAI Service documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/) for guidance on deploying the service.

# Challenge 02: Implement Document Search with Azure AI Search

### Estimated Time: 120 minutes

## Introduction:

Contoso is leveraging Azure AI Search and Azure OpenAI (GPT-3.5-Turbo) to create a document search solution that not only makes support documents easily searchable but also uses OpenAI's powerful language model to understand and process customer queries effectively. This integration will enable Contoso to provide accurate and relevant responses, thereby streamlining its support services.

Azure AI Search will be used to organize and index Contoso's large volumes of support documents, while Azure OpenAI will interpret customer queries for semantic search, improving the search results' relevance. This fusion of technologies will assist in making informed decisions and extracting vital information from unstructured data, ultimately providing a seamless information retrieval system that enhances Contoso's customer support experience.

In this challenge, you'll clone a provided repository to lay the groundwork for Azure AI Search, integrate it with Azure AI Services, and create a powerful indexer for advanced search capabilities. Finally, you'll work on refining search queries and kickstart the development of a search application that leverages both Azure AI Search and OpenAI's language model.

## Challenge Objectives:

> **Important**: When deploying services in this challenge, please make sure to use the resource group named **<inject key="Resource Group Name"/>** !

1. **Clone the Repository:**
   - Clone the repository within Visual Studio Code: `https://github.com/CloudLabsAI-Azure/mslearn-knowledge-mining.git`.
     > Hint : You can utilize the following repository, https://github.com/CloudLabsAI-Azure/mslearn-knowledge-mining.git, to explore and perform the scenarios listed below.

2. **Setup Azure Resources:**
   - Create an Azure AI Search resource with basic pricing.
   - Create an Azure AI Service with the Standard S0 SKU.
   > Note : Ensure to use the same region as the Azure AI Search resource.     
   - Create an Azure Storage Account with the Standard Tier.

3. **Prepare Document Upload:**
   - In Visual Studio Code, within the cloned repository, navigate to the 01-azure-search folder.
   - Edit the UploadDocs.cmd batch file with the required values.

4. **Execute the Upload Script:**
   - Open and examine the UploadDocs.cmd batch file using VS Code.
   - Execute the batch file to ensure that the necessary resources and files are created in Azure.
     > Hint: Begin by ensuring you have the proper credentials. This command will guide you through logging into your Azure account using the Azure CLI

5. **Data Import and Indexing:**
   - Import data for AI Search using Blob Storage.
   - Link with Azure AI Services and customize the index.
   - Create an indexer for seamless data integration.

6. **Query Indexed Documents:**
   - Tweak queries to include counts and specific fields.
   - Define search components.
   - Query the modified index to retrieve refined and targeted information.
     > Hint: Refine your queries to count results, choose specific fields, configure search components, and use the updated index for detailed and focused information 
       retrieval.

7. **Deploy & Test a Search Client Application:**
   - Update application settings and configure the web app.
   - Run the application locally to test the search functionality.
   > Hint: The application supports multiple languages; choose the one that suits your project's requirements. Adjust your application settings and configure the web application as needed. Then, run the application locally to test the search functionality before proceeding with deployment. 

     <validation step="15277a80-8b44-474a-ac19-0831c71d5fbd" />

   
## Success criteria:

To successfully complete this challenge, you must:

   - Deploy the Azure Search Service and Azure Storage Account.
   - Add data to the storage account.
   - Index the documents in Azure AI Search using the Azure portal.
   - Customize the index and configure the indexer in Azure AI Search.
   - Modify and explore search components using JSON definitions.
   - Utilize the Azure AI Search SDK to create a client application for search.
   - Run the web application locally, perform searches, and refine search results effectively.

## Additional Resources:

- Refer to [What is Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) for reference.
- [What are Indexes in Azure AI Search?](https://learn.microsoft.com/en-us/azure/search/search-what-is-an-index)
- [Searching document text at scale using Azure Cognitive Search](https://benalexkeen.com/searching-document-text-at-scale-using-azure-cognitive-search/)

# Challenge 03: Deploy NVIDIA NIM on Azure

### Estimated Time: 120 minutes

## Introduction

In the previous challenge, you successfully deployed models within the Azure OpenAI service. In this challenge, you will focus on deploying NVIDIA NIM to Azure for Challenge 3. 

NVIDIA NIM is a suite of highly optimized microservices designed to simplify and accelerate the deployment of generative AI models across the cloud, data centers, and workstations. Each NIM is packaged as a container image on a per model or model family basis, such as `meta/llama3-8b-instruct`, and can run on any NVIDIA GPU with sufficient memory. NIMs leverage a runtime that intelligently selects the best model version for the available hardware, ensuring optimal performance.

NIMs are distributed as Docker containers via the NVIDIA NGC Catalog, with each container including built-in security features, such as CVE monitoring and security scanning reports. NIMs offer flexible, scalable deployment options and are compatible with a wide range of NVIDIA GPUs, making it the fastest solution for AI inference.

You need to deploy NVIDIA NIM on one of the following services for Challenge 3:

- **Azure ML** - Deploy as a managed online endpoint
  

## Scenario

**Contoso Ltd.**, a leading technology firm, is seeking to enhance its product support operations. They receive a large volume of queries daily, resulting in longer waiting times and decreased customer satisfaction. To address this, Contoso plans to use fully optimized NVIDIA models to improve performance.

They have decided to deploy NIM to Azure alongside its Large Language Models (LLMs), such as `llama-3.1-8b-instruct`. These models excel at processing and generating human-like text, making them ideal for improving customer support.

Your task in this challenge is to create an NIM endpoint on Azure by deploying **llama-3.1-8b-instruct** NIM in the Azure environment of your choice.

### Accessing the Azure Portal

1. To access the Azure portal, open a private/incognito window in your browser and navigate to the Azure Portal.

1. On the **Sign in to Microsoft Azure** tab, you will see a login screen. Enter the following email/username and click **Next**.

   - **Email/Username:** <inject key="AzureAdUserEmail"></inject>

1. Enter the following password and click **Sign in**.

   - **Password:** <inject key="AzureAdUserPassword"></inject>

1. If you see the **Stay Signed in?** pop-up, click **No**.

1. Close any pop-ups like **You have free Azure Advisor recommendations!** or **Action Required** by clicking **Ask Later** or **Cancel** to skip the tour.

## Prerequisites

Ensure you have the following from the CloudLabs-provided integrated environment:

> **Note:** Prerequisites are pre-configured in the CloudLabs environment. If you're using your personal computer or laptop, ensure all essential prerequisites are installed.

- [Azure Subscription](https://azure.microsoft.com/en-us/free/)
- [NVAIE license](./Getting-NVAIE-License.md)

## Challenge Objectives:

Deploy **llama-3.1-8b-instruct** NIM in one of the following places:

1. **Generate NGC API KEY**

   - Login or Create Nvidia account 

   - Navigate to https://build.nvidia.com/ login using your personal email id. If not create an account.

   - Verify you are provided with 1000 free credits, each translating into one API call by clicking on **Profile**. 
   
   -  Navigate to [Nvidia](https://ngc.nvidia.com/signin) account using your credentials to proceed and Click on the **join**.
    
   - Once your account is created or you've successfully logged in.

   - You will see a pop-up. On the **Set Email Preferences For Your Services** page, you can either close it or click **Set Email Preferences** to receive updates regarding security, announcements, and maintenance for all your services.

      ![](../../Coach/media/nv8.png)

   - In the search bar, look for **Llama-3.1-8b-instruct**.

      ![](../../Coach/media/nv7.png)

   - Scroll down and select **Llama-3.1-8b-instruct**. 

      ![](../../Coach/media/nv6.png)

   - On the left-hand side, click **Get Container**.

      ![](../../Coach/media/nv5.png)

   - A pop-up will appear on the **Approval Required** page. Click **Join** for the **NVIDIA Developer Program**, and it will redirect you to the NVIDIA Developer Portal.

      ![](../../Coach/media/nv4.png)

   - On the **NVIDIA Developer Portal**, under **Integrate NIM into your application**, provide the necessary details and click **Join**.

      ![](../../Coach/media/nv3.png)

   - Navigate back to your **NVIDIA Account**. From **Organization**, click **Subscriptions** on the left. Here, you will see the **Active** status for the NVIDIA Developer Program.

      ![](../../Coach/media/nv2.png)

   - Click on **Account** at the top of the page and navigate to the **Setup** section.

      ![](../../Coach/media/nvidia4.png)

   - Click on **Generate API Key** to create a new key for accessing the necessary services.

      ![](../../Coach/media/nvidia5.png)

   - From the top, click on **+ Generate API Key** to create a new API key.

      ![](../../Coach/media/nvidia8.png)

   - Click on **Confirm** to generate your new API key.

      ![](../../Coach/media/nvidia9.png)

   - Carefully copy your generated API key, essential for accessing various services and features paste the API key in the notebook. Ensure you store it securely, as it may not be displayed again after you leave the page.

      ![](../../Coach/media/nvidia7.png)

1. **Deploy Container registries**

   - Deploy a Container registry with the following details.

     | Setting | Action |
     | --- | --- |
     | **Subscription** | Default |
     | **Resource Group** | Select the ****<inject key="Resource Group Name"/>**** resource group |
     | **Registry name** | Enter **unique name** |
     | **Location** | Choose the location as per the subscription  |
     | **Pricing plan** | **Standard** |

   - Copy the `Subscription ID` and `Container registries` name in the notepad.

3. **Setup Git Bash Environment**

   - Run the configuration in Git Bash.
   
   - Downloads the latest version of **jq**  file, a lightweight and flexible command-line JSON processor, and save it as an executable file named `jq-win64.exe` in the `/usr/bin/jq.exe` directory

     > Note: You can use https://github.com/jqlang/jq/releases url 

   - Install the az CLI by navigating to the below link:

       ```
      $ProgressPreference = 'SilentlyContinue'; Invoke-WebRequest -Uri https://azcliprod.blob.core.windows.net/msi/azure-cli-2.51.0.msi -OutFile .\AzureCLI.msi; Start-Process msiexec.exe -Wait -ArgumentList '/I AzureCLI.msi /quiet'; Remove-Item .\AzureCLI.msi
      ```

   - Install the `az ml` stable extension.

   - Clone the following repo.

      ```
      https://github.com/CloudLabsAI-Azure/nim-deploy.git
      ```

5. **Configure the config.sh**

   - Open the folder where you have cloned the repo from VS Code.
   - Update the `config.sh` file with the necessary details located in the `nim-deploy\cloud-service-providers\azure\azureml\cli` directory.
   - Update resource group as `Activate-GenAI` and image name as `nim-meta-llama-3.1-8b-instruct:latest`.

      - Detailed instructions can be found [here](https://github.com/NVIDIA/nim-deploy/tree/main/cloud-service-providers/azure/azureml/cli).

6. **Create AzureML Deployment of the NIM Container**

   - **Configuration and Login to Azure**

      - From the Git Bash change directory `nim-deploy\cloud-service-providers\azure\azureml\cli`.
      - Configuration settings defined in `config.sh`.
      - Provide a unique name for **workspace** and **acr_registry_name**, utilizing **<inject key="Deployment ID" />** as a suffix.
      -  Provide a **endpoint_name** as **llama-3-1-8b-nim-endpoint<inject key="Deployment ID" />** 
      - Provide a **deployment_name** as **llama3-1-8b-nim-dep<inject key="Deployment ID" />** 
      - This step is crucial for loading environment variables, paths, or any other configuration before running dependent commands.
      - Login to the Azure portal using the CLI command.


   - **Setup AzureML Workspace**

      - Execute the `1_set_credentials.sh` file to create a new `AzureML workspace` with the "Azure ML Secrets Reader" role assignment.
      - Verify the AzureML workspace is created in the azure-ml resource group.

   - **Store NGC API Key for Use in the AzureML Deployment**

      - Run the `2_provide_ngc_connection.sh` script to configure this and verify the connection.
   
   - **Save NIM Container in Your Container Registry**

       - Run the `./3_save_nim_container.sh` script to push the NIM container in your container registry.
       - Verify that the NIM container has been published in the container registry by checking the `Repositories`.
       - Copy the `Repositories` endpoint.

    - **Create Managed Online Endpoint**

        - Run the `./4_create_endpoint.sh` script to create a managed online endpoint.
   
   - **Role Assignment**
       
       - Provide `AcrPull` role assignment to the Machine Learning Online endpoint managed identity.

   - **Create AzureML Deployment of the NIM Container**

        - Run the `./5_create_deployment.sh` to create AzureML deployment of the NIM container.
   
   - **Verify the Connection**

     - Navigate to `Machine learning Endpoint` in `Azure Machine Learning workspace` and select the deployed **Endpoint**.
     
     - Copy the `endpoint` and `Primary Key` under Consume.
     
     - Update the `test_chat_completions.sh` file with the necessary details located in the `nim-deploy\cloud-service-providers\azure\azureml\cli` directory.
       
       > **Hint**: Use the values from the `config.sh` file.
  
       > **Note**: Ensure to append `/v1/chat/completions` to the end of the endpoint.
      
      - Run `test_chat_completions.sh` file to Verify the Connection


## Success Criteria:

- Verify that the endpoint is accessible from outside:
  
## Additional Resources:

- Refer to the [NVIDIA NIM documentation](https://docs.nvidia.com/nim/large-language-models/latest/introduction.html) for guidance on deploying the service.

### Conclusion

In this challenge, you successfully validated the Azure OpenAI service and deployed LLM models. In the next challenge, you will learn about Semantic Kernel, which is used for building intelligent apps while leveraging Azure OpenAI models.

# Challenge 04:  Deploy an AI-Powered Chat App

### Estimated Time: 90 minutes

## Introduction:

In this challenge, you'll deploy an AI-powered chat application specifically designed for Contoso Electronics. This app, built with React for the frontend and Python for the backend, showcases advanced features like chat and Q&A interfaces, all augmented by AI capabilities. It's an excellent opportunity for you to explore the integration of Azure OpenAI Service with the GPT-3.5 Turbo model and Azure Cognitive Search for efficient data indexing and retrieval.

This sample app is more than just a chat interface; it demonstrates the Retrieval-Augmented Generation pattern, offering a rich, ChatGPT-like experience over Contoso's own data. The app's features include trustworthiness evaluation of responses with citations, tracking of source content, data preparation, prompt construction, and orchestrating interaction between the ChatGPT model and Cognitive Search. You'll also find adjustable settings in the UX for experimentation and optional performance tracing and monitoring with Application Insights.

In this challenge, your task is to deploy this comprehensive chat solution for Contoso, allowing them to evaluate its capabilities and integrate it into their environment. The repository comes with sample data, representing a ready-to-use, end-to-end solution. This app is a valuable tool for Contoso's employees to inquire about company benefits, internal policies, job descriptions, and roles.

You will be using bicep to deploy the chat app. 

The chat application integrates seamlessly with different Azure services to provide an intelligent user experience. Here's a simple overview of each service used by the app:

- **App Service:** This hosts the chat app, ensuring it can respond to the prompts sent by users from the uploaded relatable data.
- **Application Insights:** It proactively monitors the app's performance, taking care of issues before they become significant.
- **Document Intelligence:** Using AI, it understands the content in uploaded documents, making user information more insightful.
- **Azure OpenAI:** Enhances the app's capabilities with natural language understanding and responses.
- **Shared Dashboard:** Acts as a central hub for team collaboration and data sharing.
- **Smart Detector Alert Rule:** Monitors the app's health and notifies the team if any issues arise.
- **Search Service:** Empowers users with dynamic and efficient search functionality within the app.
- **Log Analytics Workspace:** Tracks and analyzes app activity, offering valuable insights and logs.
- **App Service Plan:** Optimizes resource allocation for optimal app performance.
- **Storage Account:** Securely stores the data that will be used by the Azure AI Search service to provide the inputs to the chat app.

Together, these services create a responsive chat application that combines AI features, monitoring capabilities, and efficient data management, providing Contoso with an exceptional user experience.

## Architecture diagram:

![](../media/Active-image258.png)


## Prerequisites

Make sure you have the following from the CloudLabs-provided integrated environment:

> Note: Prerequisites are already set up in the CloudLabs-provided environment. If you're using your personal computer or laptop, please make sure that all necessary prerequisites are installed to complete this hackathon.


  - [Azure Subscription](https://azure.microsoft.com/en-us/free/)
  - [Azure OpenAI](https://aka.ms/oai/access) access is available with the following models:
    - gpt-35-turbo
    - text-embedding-ada-002
   - Bicep 
   - Azd 
   - Poweshell 7 

## Challenge Objectives:

> **Note**: When deploying services in this challenge, please make sure to use the resource group named rg-activategenai.

> **Important** : Start Powershell 7 +.

1. Click the Windows search button and look for **PowerShell 7-Preview**. If **PowerShell 7-Preview** is not visible, execute the following commands one by one in PowerShell ISE to install it.

   ```
   $PSVersionTable.PSVersion
   
   # Define the URL for the latest PowerShell 7 Preview MSI installer
   $url = "https://github.com/PowerShell/PowerShell/releases/download/v7.4.0-preview.2/PowerShell-7.4.0-preview.2-win-x64.msi"

   # Define the location to save the MSI file
   $output = "$env:TEMP\PowerShell-7-Preview.msi"

   # Download the MSI installer
   Invoke-WebRequest -Uri $url -OutFile $output

   # Install PowerShell 7 Preview
   Start-Process msiexec.exe -ArgumentList "/i $output /quiet" -Wait
   ```  

1. **Clone the Repository:**
   - Clone the Active Gen AI repository: `https://github.com/CloudLabsAI-Azure/azure-search-openai-demo-nvidia`.
   - Verify if Bicep is installed on your machine. If not, follow the [Bicep installation guide](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/install)).

1. **Deploy the AI-Powered Chat App:**

    - Deploy an AI-powered chat application on Azure, integrating Azure AI services and Azure Search, and ensuring it's accessible and functional post-deployment.

   - Provide the name of the environment as **activategenai** when prompted.
    
      > Hint : Begin by ensuring you have the proper credentials. This command will guide you through logging into your Azure account using the Azure Developer CLI. Once authenticated, you'll have access to your Azure resources.
    
      > Hint : Initialize your project with a specific template. This command will help you set up your project environment

      > Hint : Launch your project into action. This command will deploy your application to Azure, setting up all necessary resources and configurations automatically.

      > Note : Ensure to re-run in case of any deployment failure with Storage Account.

1. **Deploying with NVIDIA NIM**

    - Along with OpenAI Large Language Models (LLMs), NVIDIA NIM along with Meta Llama 3.1 8B can be used for ChatCompletion requests.This document outlines the steps to configure the app to use NVIDIA NIM.
    - Follow the given instructions here: `https://github.com/CloudLabsAI-Azure/azure-search-openai-demo-nvidia/blob/main/docs/deploy_nvidia_nim.md`

## Success Criteria:

- Successful deployment of the Chat App.
- validate if the following services are successfully deployed in the RG (Resource Group).
  - App Service
  - Document Intelligence
  - Azure OpenAI
  - Shared Dashboard
  - Smart Detector Alert Rule
  - Search Service
  - Log Analytics Workspace
  - App Service Plan
  - Storage Account
- Validate if the data is populated into the storage container named `content`.
- The Chat app should be accessible using the Azure App service.

## Additional Resources:

-  Refer to the  [Azure Search OpenAI demo GitHub repository](https://github.com/cmendible/azure-search-openai-demo) for detailed information on the architecture.
-  [Azure copilot](https://learn.microsoft.com/en-us/azure/copilot/overview)

# Challenge 05: Serverless Document Batch Processing 

### Estimated Time: 90 minutes

## Introduction:

Welcome to a pivotal challenge where Contoso Ltd. aims to enhance its AI-powered chat app with a robust document processing system. This challenge focuses on creating a serverless solution for processing new documents, translating them as needed, and seamlessly storing them into Azure AI Search. This system will ensure that these documents are continuously available for consumption by Azure OpenAI, enhancing the chat app's knowledge base and response accuracy.

Building on your previous achievements in load-balancing Azure OpenAI resources, you will now embark on a journey to streamline document processing. This involves setting up a translation service, creating a serverless architecture for batch processing using Azure services, and leveraging technologies like Form Recognizer and Azure AI Search. Your task is to ensure that newly added documents are promptly processed, analyzed, and indexed, making them readily available for the chat app's AI to utilize.

This challenge unfolds in three main stages: language translation, serverless document batch processing using Azure services, and leveraging advanced features like Form Recognizer and AI search. We kick things off by translating files to meet language requirements. Next, you deploy a serverless architecture, utilizing Azure services, for efficient batch processing of documents. You train and test our model, establish a pipeline to convert documents into a Form Recognizer format, and bring in Azure's AI search service to verify the presence of specific documents in the processed dataset from where they can be used by Azure OpenAI. 

You will utilize the Form Recognizer Service and the Business Process Automation (BPA) Accelerator to build pipelines across various Azure services, creating a seamless document processing solution. This challenge is a step towards realizing an AI solution that can adapt and grow with Contoso's business needs.

## Challenge Objectives:

> **Important**: When deploying services in this challenge, please make sure to use the resource group named **<inject key="Resource Group Name"/>** 

1. **Fork the repository and generate a GitHub Personal Access Token (PAT).**

   - Fork the repo Business Process Automation repository into your GitHub: `https://github.com/CloudLabs-MOC/business-process-automation`.
   - Generate a **GitHub Personal Access Token (PAT)** with Workflow Level Token.


2. **Deploy Azure Infrastructure in the Azure Portal**:

   - Click on the "Deploy to Azure" button (TODO):

     [![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FCloudLabs-MOC%2Fbusiness-process-automation%2Fmain%2Ftemplates%2Foneclickoai.json)

   - Update the repository token and forked Git repository URL while leaving all other settings unchanged.

      > **Note:** Ensure the Primary region is set to **EASTUS2**.
      
3. **Setup Azure Blob Storage.**
   - Create mandatory source and target containers in Azure Blob Storage for document processing by granting blob access.


4. **Initialize the C#/.NET Environment for Document Processing:**
   - Set up a C#/.NET project in Visual Studio for document translation using .Net Version 7.
   - Install the necessary packages, including Newtonsoft.Json.


5. **Translate Documents and Run the Application:**
   - Implement document translation code in the C#/.NET project.
   - Execute the application to translate all documents in the storage container.

      > **Note:** You can find the documents in C:\LabFiles\Documents.


   <validation step="e7cc8d8f-1ac3-46be-9f16-d5a492ff6147" />

**Using Doc Intelligence:**
> **Important**: When deploying services in this challenge, please make sure to use the resource group named **<inject key="Resource Group Name"/>**  

1. **Using an Azure Document Intelligence (Form Recognizer) resource:**
    - Navigate to Azure AI services and utilize the Azure Document Intelligence (Form Recognizer) resource.
    - Upload and label training documents to train the Azure Document Intelligence (Form Recognizer) model.

      > **Note:** You can find the documents in C:\LabFiles\Documents.

1. **Build a New Pipeline with a Custom Model Module in BPA:**
    - Utilize the trained Azure Document Intelligence  to create a new pipeline in BPA.
    - Configure the pipeline for efficient document processing and integration with Azure Cognitive Search.

      > **Hint:** Utilize static web app.

1. **Configure Azure AI Search:**
    - Connect to Azure Blob Storage and configure data import and indexing.
    - Set up an indexer for organized data retrieval.


2. **Update the Azure OpenAI Model to use the Azure AI Search**
    - Update your existing Azure OpenAI model deployment to connect to the newly created AI Search index and test using the Azure OpenAI Playground.
      
## Success Criteria:

- Successful translation of documents and storage in the Azure Blob Storage target container.
- Effective setup and utilization of the Form Recognizer resource and BPA pipeline.
- Proper configuration of Azure Cognitive Search for processed documents.
- Validation of document processing and search functionality using the Sample Search Application in BPA.

## Additional Resources:

- Refer to [document translation](https://learn.microsoft.com/en-us/azure/ai-services/translator/document-translation/quickstarts/document-translation-rest-api?pivots=programming-language-csharp#code-sample) for sample code that will be used for document translation using C#.
- Refer to [Document Translation operations](https://learn.microsoft.com/en-us/azure/ai-services/translator/document-translation/reference/rest-api-guide) to understand the REST APIs that we utilize for document translation.

# Challenge-06: Implement Monitoring and Logging of Azure OpenAI Using API Management Service

### Estimated Time: 90 minutes

## Introduction:

Building on the success of enhancing Contoso's AI-powered chat app with serverless document processing, your next objective is to operationalize these Azure OpenAI solutions with robust monitoring and logging mechanisms. In this challenge, you will delve into the intricacies of setting up and analyzing advanced monitoring systems using the Azure API Management Service and Log Analytics workspace. This is a crucial step in ensuring the smooth operation and maintenance of the AI solutions you've developed, providing valuable insights into system performance and user interactions.

Your task is to implement comprehensive monitoring for the Azure OpenAI service, leveraging diagnostic settings and Kusto queries for in-depth log analysis. Additionally, you'll be integrating the API Management Service to oversee the chat message completions and further analyze the prompts and outputs. This level of monitoring is essential for Contoso to maintain a high-quality, efficient, and user-friendly AI chat application.

## Challenge Objectives:

> **Important**: When deploying services in this challenge, please make sure to use the resource group named **rg-activategenai**

1. **Monitoring the Azure OpenAI Service:**
   - Set up diagnostic settings for the existing Azure OpenAI services.
   - Conduct log analysis utilizing Kusto Queries to monitor the service's performance and usage.
     
2. **Monitoring OpenAI prompts using Azure API Management:**
   - Utilize Kusto queries within API Management for comprehensive log analysis, focusing on chat message completions and prompt interactions.

     <validation step="bc6cc0b0-ab0e-4b2f-9e3a-1b1836b20e28" />
  
## Success Criteria:

Participants will be evaluated based on the following criteria:

1. Successfully configure the Azure OpenAI service with appropriate diagnostic settings and analyze its logs using Kusto Queries.
2. Effectively create and configure Azure API Management, ensuring clear visibility of logs and OpenAI prompts through detailed Kusto Query analysis.

## Additional Resources:

- Refer to [How to Configure Azure API Management Service](https://github.com/Azure-Samples/openai-python-enterprise-logging/blob/main/README.md) for detailed information.
- Refer to this video about [Logging & Monitoring Everything in Azure OpenAI with API Management Service](https://github.com/Azure-Samples/openai-python-enterprise-logging/blob/main/README.md).
- Refer to the [Kusto Queries Tutorial](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-tutorial) for detailed information.
